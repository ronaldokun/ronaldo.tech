{
  
    
        "post0": {
            "title": "Tensores e Multiplicação de Matrizes",
            "content": ". Um escalar ( número ), um vetor, uma matriz ou qualquer estrutura n-dimensional são simplesmente tensores de diferentes ordens. Um escalar é um tensor de ordem (ou dimensão) 0, um vetor é um tensor de ordem 1, uma matriz é um tensor de ordem 2. Uma matriz &quot;cúbica&quot; é um tensor de ordem 3 e assim por diante.Desse modo daqui em diante nos referimos a tudo simplesmente como &quot;tensores&quot;, sejam eles vetores, matrizes ou objetos de dimensão superior. . Vamos importar as dependências necessárias, pytorch e numpy. . . Tip: Utilize o link acima Open in Colab para explorar e editar o código dessa postagem direto do Navegador. Nele todas as dependências já estão instalada. Não se preocupe de instalar e configurar bibliotecas num primeiro momento. . import torch import numpy as np . Criando alguns tensores b&#225;sicos . Para criar um escalar, basta fornecer um número ao construtor. . escalar = torch.tensor(5) escalar . tensor(5) . escalar.dtype . torch.int64 . No entanto o uso de tensores de tipo inteiro é limitado, o padrão é criarmos tensores do tipo ponto flutuante ( float ). . escalar = torch.tensor(5.) escalar . tensor(5.) . escalar.dtype . torch.float32 . 5. é uma abreviação de 5.0. . Para checarmos a dimensão do tensor, usamos o método dim . escalar.dim() . 0 . Se quisermos recuperar o tensor escalar como um número python, usamos o método item, este metódo também é válido para tensores não escalares que contém somente 1 elemento: . escalar.item() . 5.0 . x = torch.tensor([[2.]]) x.item() . 2.0 . x = torch.randn(3, 3) ; x . tensor([[ 1.2155, -0.4055, 0.3777], [-0.1294, 1.0666, 0.4260], [ 1.1780, -1.4406, 0.1562]]) . Para criarmos um vetor, basta fornecermos qualquer objeto iterável do python, como lista, tuplas, range, etc... . vetor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) vetor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . . Important: Uma restrição importante é que os dados de um tensor devem ter tipo único, não podemos ter um tensor com inteiros, floats e booleanos por exemplo . vetor = torch.tensor([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) vetor . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.]) . No caso acima fornecemos somente o primeiro número como float: 0. e o Pytorch converteu todos os demais para o tipo float . vetor = torch.tensor([x for x in range(12)]) vetor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . vetor = torch.tensor(range(12)) vetor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . Para checar a dimensão do tensor, poder utilizar o atributo shape ou o método size . vetor.shape . torch.Size([12]) . vetor.size() . torch.Size([12]) . print(f&#39;Dimensão do Vetor: {vetor.dim()}&#39;) . Dimensão do Vetor: 1 . Pytorch é fortemente integrado com o numpy, inclusive empresta muito da API utilizada por este. Se você possui alguma familiaridade com numpy facilmente consegue compreender e escrever código em Pytorch. . Criamos um array com numpy . array = np.array([[0., 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]) array . array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) . Para transformarmos em tensor, existem várias formas como passar o array diretamente ao construtor torch.tensor, no entanto o construtor cria um cópia do array original. Outros métodos como: torch.from_numpy e torch.as_tensor compartilham a memória e a modificação efetuada em 1 é refletida no outro. . matrix = torch.from_numpy(array) matrix . tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=torch.float64) . matrix[0,0] = -1 . array . array([[-1., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) . . Note: Para acessar os elementos de um tensor, é usada a mesma indexação já conhecida de objetos python e numpy arrays . Se quisermos recuperar o array novamente, temos o método numpy . matrix.numpy() . array([[-1., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) . Mudando o formato de um tensor . Frequentemente temos que adaptar o formato dos tensores para efetuar diversas operações, isso é feito com o método reshape. Vamos transformar o tensor unidimensional anteriormente definido com 12 elementos em diferentes formatos. Isso é possível desde que o resultado contenha o mesmo número de elementos. . vetor = vetor.reshape(3,4) vetor . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . vetor.shape . torch.Size([3, 4]) . vetor.dim() . 2 . Se não informarmos 1 dimensão e colocarmos -1 em seu lugar, o Pytorch infere a quantidade de elementos das demais. . vetor = vetor.reshape(3,-1) vetor . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . vetor.reshape(-1, 4) . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . vetor = vetor.reshape(-1, 2, 2) vetor . tensor([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11]]]) . vetor.shape . torch.Size([3, 2, 2]) . vetor.dim() . 3 . Se informarmos uma quantidade de elementos divergentes é retornado um RuntimeError . vetor.reshape(3, 4, 4) . RuntimeError Traceback (most recent call last) &lt;ipython-input-31-49750a02386c&gt; in &lt;module&gt; -&gt; 1 vetor.reshape(3, 4, 4) RuntimeError: shape &#39;[3, 4, 4]&#39; is invalid for input of size 12 . Normalmente criamos dados à partir de algum dado armazenado, por exemplo uma imagem é um tensor com 3 dimensões: Linhas x Colunas x Cores. No entanto para vários tensores comumente utilizados, existe diversos métodos do Pytorch que possibilita a criação automática deles. Vejamos alguns: . Tensor Nulo . torch.zeros((3,4)) . tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . Tensor Unitário . torch.ones(3, 4) . tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . Tensor Identidade . torch.eye(3,3) . tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . Tensor com valores amostrados de alguma distribuição . t1 = torch.randn((3,4)) ; t1 . tensor([[ 0.6482, -1.2590, 1.1658, 0.9287], [-1.2208, 0.8042, 1.0299, 0.2370], [-1.4051, 0.1101, -0.3225, 0.1291]]) . Como a distribuição normal com média 0 e desvio padrão 1 e tão comum existe esse método para criá-la rapidamente somente fornecendo as dimensões desejadas. . t1.mean() . tensor(0.0705) . t1.std() . tensor(0.9307) . Para outros valores de média e desvio padrão utilize: . x = torch.normal(2,4, (3,4)) . x.mean() . tensor(1.2909) . x.std() . tensor(3.7982) . Para outras distribuições consulte a documentação do Pytorch. . Empilhar Tensores . É comum precisarmos combinar diferentes tensores ao longo de algum eixo. Isso normalmente significa criarmos uma dimensão adicional e mantermos as demais dimensões. Sejam 2 tensores 3x4: . x = torch.arange(12).reshape(3,4).float() y = torch.Tensor([[2,1,4,3], [1,2,3,4], [4,3,2,1]]).float() . x,y . (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]])) . Vamos empilhá-los à partir da primeira dimensão . z = torch.stack([x,y], dim=0) ; z . tensor([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], [[ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]]) . z.shape . torch.Size([2, 3, 4]) . Vemos que foi criada uma dimensão adicional e colocado no início, dado o parâmetro dim=0. Caso quiséssemos empilhar os tensores numa dimensão distinta. . z = torch.stack([x,y], dim=1) ; z . tensor([[[ 0., 1., 2., 3.], [ 2., 1., 4., 3.]], [[ 4., 5., 6., 7.], [ 1., 2., 3., 4.]], [[ 8., 9., 10., 11.], [ 4., 3., 2., 1.]]]) . z.shape . torch.Size([3, 2, 4]) . z = torch.stack([x,y], dim=2) ; z . tensor([[[ 0., 2.], [ 1., 1.], [ 2., 4.], [ 3., 3.]], [[ 4., 1.], [ 5., 2.], [ 6., 3.], [ 7., 4.]], [[ 8., 4.], [ 9., 3.], [10., 2.], [11., 1.]]]) . z.shape . torch.Size([3, 4, 2]) . Criar tensores booleanos &#224; partir de compara&#231;&#245;es . Ao compararmos dois tensores distintos, o resultado é um tensor de mesma dimensão cujos elementos são os valores booleanos (Verdadeiro ou Falso) da comparação termo a termo . z = x == y z . tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) . Como em python, ao tentarmos fazer operações numéricas com valores booleanos, temos True == 1 e False == 0. Assim ao somarmos os valores do tensor acima temos o resultado 2. . z.sum() . tensor(2) . Opera&#231;&#245;es Elemento a Elemento . As seguintes operações em tensores são efetuadas elemento a elemento, isto é, a operação tem como resultado um tensor da mesma forma, porém com a operação efetuada nos elementos de mesmo índice. Isso normalmente exige que os tensores tenham dimensões iguais. . x = torch.Tensor([1,2,4,8]) y = torch.Tensor([2,2,2,2]) . Adição . x+y . tensor([ 3., 4., 6., 10.]) . Subtração . x-y . tensor([-1., 0., 2., 6.]) . Multiplicação (Elemento a Elemento) . Warning: Não confunda com a multiplicação matricial, esta retorna uma matriz. . x*y . tensor([ 2., 4., 8., 16.]) . Exponencial . x**y . tensor([ 1., 4., 16., 64.]) . Broadcasting . Uma tradução livre para o termo Broadcasting é transmissão. Broadcasting ocorre quando tentamos realizar algumas operações aritméticas, como as operações elemento a elemento vistas no parágrafo anterior, em tensores com dimensões distintas. Se a operação suporta broadcasting e a dimensão dos tensores são compatíveis com algumas regras, essas dimensões são automaticamente expandidas, i.e. os valores das dimensões menores são transmitidos para as dimensões expandidas sem que haja cópia desnecessária dos mesmos. . As duas regras semânticas básicas que devem ser obedecidas pelos tensores para que seja possível o broadcasting são: . Cada tensor precisa ter ao menos 1 dimensão | Ao alinharmos as dimensões de ambos tensores elas precisam obedecer um dos casos: Serem iguais | Uma delas possuir o valor 1 | Uma delas não existir | | Isso fica mais claro com alguns exemplos. . O caso mais simples de broadcasting é ao multiplicarmos um escalar a um tensor.Seja o exemplo de multiplicação de tensores elemento a elemento: . a = torch.tensor([1., 2, 3]) b = torch.tensor([2., 2, 2]) a * b . tensor([2., 4., 6.]) . Podemos obter o mesmo resultado multiplicando por um escalar . b = 2.0 a * b . tensor([2., 4., 6.]) . O resultado é o mesmo da operação anterior, foi realizado o broadcasting. Vamos checar as regras: . Num primeiro momento parece que violamos a regra 1, porque mesmo um número sendo promovido a um tensor escalar, este por definição possui dimensão nula. No entanto neste caso o escalar é equivalente a um vetor com um único elemento. Assim a regra não é violada e podemos considerar o escalar como um vetor linha - 1x1 | Quanto à regra 2: 2ii: Ao alinharmos as colunas do tensor b com o tensor a: A 1ª coluna em b possui a dimensão 1 | 2iii: Para as colunas 2 e 3 de a as colunas em b não existem | . | Assim ao percorrermos as dimensões de ambos tensores todas as regras do broadcasting são obedecidas. Não se preocupe em decorar essas regras. Isso foi somente para ilustrar, com alguma prática isso se torna automático. . Conceitualmente o vetor é esticado de 1 para 3 colunas e o valor da coluna 1 em b é transmitido para as outras duas colunas, daí o termo broadcasting | Isso é conceitual somente porque o valor não é duplicado pelo Pytorch, inclusive é mais eficiente quanto à memória do que a operação anterior porque um escalar ocupa menos espaço que um vetor. | . . Um segundo exemplo com uma matriz ( 2 dimensões ) e um vetor ( 1 dimensão ) . a = torch.tensor([[ 0.0, 0.0, 0.0], [10.0, 10.0, 10.0], [20.0, 20.0, 20.0], [30.0, 30.0, 30.0]]) b = torch.tensor([1.0, 2.0, 3.0]) a + b . tensor([[ 1., 2., 3.], [11., 12., 13.], [21., 22., 23.], [31., 32., 33.]]) . Nesse caso temos as dimensões a:4x3 e b:1x3: Ao percorrermos as dimensões de ambos temos . A segunda dimensão é igual a 3: 2i | A primeira dimensão de um dos tensores é 1: 2ii | . Assim a única linha do vetor b é replicada outras 3 vezes para que o resultado tenha as mesmas dimensão da matriz a . . O broadcasting em vários casos não é intuitivo, esses foram dois exemplos super simples e somente a utilização constante e botar a mão na massa com vários exemplos é que torna o conceito um pouco mais claro. Consulte esse link a seguir para um tutorial detalhado sobre broadcasting . Multiplica&#231;&#227;o de Matrizes . A definição de Multiplicação de 2 matrizes A e B é: . A fórmula para cada termo da matriz produto C é: $$C_{ij} = sum_kA_{ik}B_{kj}$$ . Em código no geral é mais fácil visualizar, vamos criar 2 matrizes de tamanho razoável para analisarmos a eficiência dos cálculos: . a = torch.randn(56, 112) b = torch.randn(112, 56) . a_linhas, a_colunas = a.shape b_linhas, b_colunas = b.shape . Para que seja possível multiplicar ambas as matrizes precisamos que o número de colunas da matriz a deve ser igual ao número de linhas da coluna b. . a_colunas == b_linhas . True . Vamos inicialmente criar o vetor produto vazio: . c = torch.zeros(a_linhas, b_colunas) . Na definição básica da multiplicação de matriz temos 3 loops: . for i in range(a_linhas): for j in range(b_colunas): for k in range(a_colunas): # ou b_linhas c[i,j] += a[i,k] * b[k,j] . Vamos colocar a definição acima numa função, assim não repetimos código, podemos reutilizá-lo e mensurar o tempo médio de execução . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas, &quot;O número de colunas da matriz {a_cols} deve ser igual a {b_linhas}&quot; c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): for j in range(b_cols): for k in range(a_cols): # ou b_linhas c[i,j] += a[i,k] * b[k,j] return c . Vamos calcular quanto tempo leva esse cálculo utilizando a definição básica com 3 loops. . %time c=matmul(a, b) . CPU times: user 13.4 s, sys: 0 ns, total: 13.4 s Wall time: 13.4 s . Vemos que essas matrizes, muito pequenas para os parâmetros modermos de Deep Learning, o cálculo já leva um tempo enorme, o que a torna insustentável. Na prática os &quot;loops&quot; são eliminados, e métodos eficientes de &quot;vetorização&quot; são utilizados. Vamos testar como podemos otimizar o código somente utilizando o broadcasting visto . Podemos eliminar o índice k, usando a multiplicação elemento a elemento, no lugar do índice k colocamos :, o que significa aplicarmos em todo eixo referenciado. A seguir utilizamos o método sum no eixo. Isso resulta na mesma operação efetuada anteriormente com o loop em k . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas, &quot;O número de colunas da matriz {a_cols} deve ser igual a {b_linhas}&quot; c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): for j in range(b_cols): c[i,j] = (a[i,:] * b[:,j]).sum() return c . %time c=matmul(a, b) . CPU times: user 137 ms, sys: 139 µs, total: 137 ms Wall time: 136 ms . Podemos otimizar mais e eliminar o loop no índice j. No entanto não podemos simplesmente eliminá-lo, porque senão viola as regras acima de broadcasting, seja por exemplo o índice i=0 e j=0: . a[0, :] * b . RuntimeError Traceback (most recent call last) &lt;ipython-input-67-c7015e3e45b9&gt; in &lt;module&gt; -&gt; 1 a[0, :] * b RuntimeError: The size of tensor a (112) must match the size of tensor b (56) at non-singleton dimension 1 . a[0, :].shape . torch.Size([112]) . b.shape . torch.Size([112, 56]) . O vetor a indexado possui somente uma dimensão com 4 elementos e b possui 2 dimensões no formato 4x3. Para nos adequar às regras de broadcasting temos que criar um eixo adicional para assim atender à regra 2i: . Para tal podemos indexar o eixo adicional com o valor especial None ou usar o método unsqueeze() . a[0, :, None].shape . torch.Size([112, 1]) . a[0].unsqueeze(-1).shape . torch.Size([112, 1]) . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0) return c . %time c=matmul(a, b) . CPU times: user 3.98 ms, sys: 0 ns, total: 3.98 ms Wall time: 3.5 ms . Mas isso foi somente para ilustrar os conceitos, na prática usamos a Implementação Otimizada do Pytorch em C++ . %time c= a.matmul(b) . CPU times: user 168 µs, sys: 3.9 ms, total: 4.07 ms Wall time: 3.39 ms . Um método equivalente é usar o operador @ . %time c= a@b . CPU times: user 143 µs, sys: 5 µs, total: 148 µs Wall time: 154 µs . Isso foi somente uma pincelada super básica sobre tensores e alguns conceitos e operações básicas sobre eles e uma ilustração em como implementar multiplicação de matrizes de forma básica e mais otimizada para ilustrar tais conceitos. Por fim simplesmente utilizamos a implementação nativa do Pytorch quando formos de fato utilizar tais operações .",
            "url": "https://ronaldo.tech/pytorch/basics/2020/05/23/Tensores.html",
            "relUrl": "/pytorch/basics/2020/05/23/Tensores.html",
            "date": " • May 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre mim",
          "content": "Meu nome é Ronaldo S.A. Batista, pai da Melinda, marido da Priscila. Formei-me em Ciências Moleculares na USP, mas as tribulações da vida me impediram de seguir o plano de ser um acadêmico multidisciplinar. Desde então atuo como fiscal de serviços públicos na ANATEL, especificamente no monitoramento do espectro de radiofrequência. Além das tarefas de monitoramente implemento soluções de análises de dados em python . Sou muito grato por tudo que essa posição me proveu e continua provendo. . No entanto, nunca abandonei o estudo de Física, Matemática e Programação, em vez disso os tornei um hobby. . Programar é mais um hobby que trabalho, dado a oportunidade rara de criar algo. . Sempre fui vidrado em como criar melhores hábitos e qual a melhor maneira de atingir nossos objetivos, por que a vida é curta e se pudermos torná-la mais eficiente vale a tentativa. . Esse blog é uma tentativa de criar o hábito de escrever e consolidar o conhecimento, se for útil para uma pessoa sequer já é um bônus. .",
          "url": "https://ronaldo.tech/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ronaldo.tech/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}