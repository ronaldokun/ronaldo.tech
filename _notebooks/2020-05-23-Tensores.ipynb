{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pt"
   },
   "source": [
    "# Tensores e Multiplicação de Matrizes\n",
    "> \"O Tensor é a estrutura de dados básica para implementarmos desde redes neurais simples até Arquiteturas Complexas de Aprendizagem Profunda (i.e. Deep Learning ). O cerne de qualquer biblioteca de Aprendizagem Profunda, seja Tensorflow, Pytorch, Theano etc.., é a manipulação e processamento de Tensores.\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [pytorch, basics]\n",
    "- image: images/copied_fom_nb/images/multiplication.png\n",
    "- hide_binder_badge: true\n",
    "- author: Ronaldo S.A. Batista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pt"
   },
   "source": [
    "![](images/multiplication.png \"Crédito: Gayatri Malhotra on Unsplash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um escalar ( número ), um vetor, uma matriz ou qualquer estrutura n-dimensional são simplesmente tensores de diferentes ordens. Um escalar é um tensor de ordem (ou dimensão) 0, um vetor é um tensor de ordem 1, uma matriz é um tensor de ordem 2. Uma matriz \"cúbica\" é um tensor de ordem 3 e assim por diante.Desse modo daqui em diante nos referimos a tudo simplesmente como \"tensores\", sejam eles vetores, matrizes ou objetos de dimensão superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos importar as dependências necessárias, `pytorch` e `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Utilize o link acima `Open in Colab` para explorar e editar o código dessa postagem direto do Navegador. Nele todas as dependências já estão instalada. Não se preocupe de instalar e configurar bibliotecas num primeiro momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando alguns tensores básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar um escalar, basta fornecer um número ao construtor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalar = torch.tensor(5)\n",
    "escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalar.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto o uso de tensores de tipo inteiro é limitado, o padrão é criarmos tensores do tipo ponto flutuante ( `float` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalar = torch.tensor(5.)\n",
    "escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalar.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` é uma abreviação de `5.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para checarmos a dimensão do tensor, usamos o método `dim`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalar.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos recuperar o tensor escalar como um número python, usamos o método `item`, este metódo também é válido para tensores não escalares que contém somente 1 elemento: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalar.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[2.]])\n",
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2155, -0.4055,  0.3777],\n",
       "        [-0.1294,  1.0666,  0.4260],\n",
       "        [ 1.1780, -1.4406,  0.1562]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3) ; x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criarmos um vetor, basta fornecermos qualquer objeto iterável do python, como lista, tuplas, range, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) \n",
    "vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: Uma restrição importante é que os dados de um tensor devem ter tipo único, não podemos ter um tensor com inteiros, floats e booleanos por exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor = torch.tensor([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) \n",
    "vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso acima fornecemos somente o primeiro número como `float: 0.` e o Pytorch converteu todos os demais para o tipo `float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor  = torch.tensor([x for x in range(12)])\n",
    "vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor = torch.tensor(range(12))\n",
    "vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para checar a dimensão do tensor, poder utilizar o atributo `shape` ou o método `size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do Vetor: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensão do Vetor: {vetor.dim()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` é fortemente integrado com o `numpy`, inclusive empresta muito da API utilizada por este. Se você possui alguma familiaridade com numpy facilmente consegue compreender e escrever código em Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos um array com numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([[0., 1, 2, 3], \n",
    "                  [4, 5, 6, 7], \n",
    "                  [8, 9, 10, 11]])\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para transformarmos em tensor, existem várias formas como passar o array diretamente ao construtor `torch.tensor`, no entanto o construtor cria um cópia do array original. Outros métodos como: `torch.from_numpy` e `torch.as_tensor` compartilham a memória e a modificação efetuada em 1 é refletida no outro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.from_numpy(array) \n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix[0,0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Para acessar os elementos de um tensor, é usada a mesma indexação já conhecida de objetos python e numpy arrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos recuperar o array novamente, temos o método `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudando o formato de um tensor\n",
    "Frequentemente temos que adaptar o formato dos tensores para efetuar diversas operações, isso é feito com o método `reshape`. Vamos transformar o tensor unidimensional anteriormente definido com 12 elementos em diferentes formatos. Isso é possível desde que o resultado contenha o mesmo número de elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor = vetor.reshape(3,4)\n",
    "vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se não informarmos 1 dimensão e colocarmos `-1` em seu lugar, o Pytorch infere a quantidade de elementos das demais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor = vetor.reshape(3,-1)\n",
    "vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3]],\n",
       "\n",
       "        [[ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor = vetor.reshape(-1, 2, 2)\n",
    "vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se informarmos uma quantidade de elementos divergentes é retornado um `RuntimeError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 4, 4]' is invalid for input of size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-49750a02386c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvetor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 4, 4]' is invalid for input of size 12"
     ]
    }
   ],
   "source": [
    "vetor.reshape(3, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente criamos dados à partir de algum dado armazenado, por exemplo uma imagem é um tensor com 3 dimensões: Linhas x Colunas x Cores. No entanto para vários tensores comumente utilizados, existe diversos métodos do Pytorch que possibilita a criação automática deles. Vejamos alguns:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Unitário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Identidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor com valores amostrados de alguma distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6482, -1.2590,  1.1658,  0.9287],\n",
       "        [-1.2208,  0.8042,  1.0299,  0.2370],\n",
       "        [-1.4051,  0.1101, -0.3225,  0.1291]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn((3,4)) ; t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a distribuição normal com média 0 e desvio padrão 1 e tão comum existe esse método para criá-la rapidamente somente fornecendo as dimensões desejadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0705)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9307)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para outros valores de média e desvio padrão utilize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.normal(2,4, (3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2909)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7982)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para outras distribuições consulte a documentação do Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empilhar Tensores\n",
    "É comum precisarmos combinar diferentes tensores ao longo de algum eixo. Isso normalmente significa criarmos uma dimensão adicional e mantermos as demais dimensões. Sejam 2 tensores 3x4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12).reshape(3,4).float()\n",
    "y = torch.Tensor([[2,1,4,3], [1,2,3,4], [4,3,2,1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[2., 1., 4., 3.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [4., 3., 2., 1.]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos empilhá-los à partir da primeira dimensão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.stack([x,y], dim=0) ; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que foi criada uma dimensão adicional e colocado no início, dado o parâmetro `dim=0`. Caso quiséssemos empilhar os tensores numa dimensão distinta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 2.,  1.,  4.,  3.]],\n",
       "\n",
       "        [[ 4.,  5.,  6.,  7.],\n",
       "         [ 1.,  2.,  3.,  4.]],\n",
       "\n",
       "        [[ 8.,  9., 10., 11.],\n",
       "         [ 4.,  3.,  2.,  1.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.stack([x,y], dim=1) ; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.],\n",
       "         [ 1.,  1.],\n",
       "         [ 2.,  4.],\n",
       "         [ 3.,  3.]],\n",
       "\n",
       "        [[ 4.,  1.],\n",
       "         [ 5.,  2.],\n",
       "         [ 6.,  3.],\n",
       "         [ 7.,  4.]],\n",
       "\n",
       "        [[ 8.,  4.],\n",
       "         [ 9.,  3.],\n",
       "         [10.,  2.],\n",
       "         [11.,  1.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.stack([x,y], dim=2) ; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar tensores booleanos à partir de comparações\n",
    "Ao compararmos dois tensores distintos, o resultado é um tensor de mesma dimensão cujos elementos são os valores booleanos (Verdadeiro ou Falso) da comparação termo a termo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x == y \n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em python, ao tentarmos fazer operações numéricas com valores booleanos, temos `True == 1` e `False == 0`. Assim ao somarmos os valores do tensor acima temos o resultado `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações Elemento a Elemento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seguintes operações em tensores são efetuadas _elemento a elemento_, isto é, a operação tem como resultado um tensor da mesma forma, porém com a operação efetuada nos elementos de mesmo índice. Isso normalmente exige que os tensores tenham dimensões iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1,2,4,8])\n",
    "y = torch.Tensor([2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4.,  6., 10.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0.,  2.,  6.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicação (Elemento a Elemento)\n",
    "> Warning: Não confunda com a multiplicação matricial, esta retorna uma matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  8., 16.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4., 16., 64.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "Uma tradução livre para o termo _Broadcasting_ é transmissão. _Broadcasting_ ocorre quando tentamos realizar algumas operações aritméticas, como as operações elemento a elemento vistas no parágrafo anterior, em tensores com dimensões distintas. Se a operação suporta _broadcasting_ e a dimensão dos tensores são compatíveis com algumas regras, essas dimensões são automaticamente expandidas, i.e. os valores das dimensões menores são _transmitidos_ para as dimensões expandidas sem que haja cópia desnecessária dos mesmos. \n",
    "\n",
    "As duas regras semânticas básicas que devem ser obedecidas pelos tensores para que seja possível o _broadcasting_ são:\n",
    " 1. Cada tensor precisa ter ao menos 1 dimensão\n",
    " 2. Ao alinharmos as dimensões de ambos tensores elas precisam obedecer um dos casos:\n",
    "  1. Serem iguais\n",
    "  2. Uma delas possuir o valor 1\n",
    "  3. Uma delas não existir\n",
    " \n",
    "Isso fica mais claro com alguns exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O caso mais simples de _broadcasting_ é ao multiplicarmos um escalar a um tensor.Seja o exemplo de multiplicação de tensores elemento a elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1., 2, 3])\n",
    "b  = torch.tensor([2., 2, 2])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obter o mesmo resultado multiplicando por um escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 2.0\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado é o mesmo da operação anterior, foi realizado o _broadcasting_. Vamos checar as regras:\n",
    " 1. Num primeiro momento parece que violamos a regra `1`, porque mesmo um número sendo promovido a um tensor escalar, este por definição possui dimensão nula. No entanto neste caso o escalar é equivalente a um vetor com um único elemento. Assim a regra não é violada e podemos considerar o escalar como um vetor linha - 1x1  \n",
    " 2. Quanto à regra 2:\n",
    "  - `2ii`: Ao alinharmos as colunas do tensor `b` com o tensor `a`: A 1ª coluna em `b` possui a dimensão 1\n",
    "  - `2iii`: Para as colunas 2 e 3 de `a` as colunas em `b` não existem\n",
    "  \n",
    "Assim ao percorrermos as dimensões de ambos tensores todas as regras do _broadcasting_ são obedecidas. Não se preocupe em decorar essas regras. Isso foi somente para ilustrar, com alguma prática isso se torna automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conceitualmente o vetor é esticado de 1 para 3 colunas e o valor da coluna 1 em `b` é transmitido para as outras duas colunas, daí o termo _broadcasting_ \n",
    "- Isso é conceitual somente porque o valor não é duplicado pelo Pytorch, inclusive é mais eficiente quanto à memória do que a operação anterior porque um escalar ocupa menos espaço que um vetor.\n",
    "\n",
    "![](https://numpy.org/devdocs/_images/theory.broadcast_1.gif \"Credit: Imagem retirada da documentação Numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um segundo exemplo com uma matriz ( 2 dimensões ) e um vetor ( 1 dimensão )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [11., 12., 13.],\n",
       "        [21., 22., 23.],\n",
       "        [31., 32., 33.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[ 0.0,  0.0,  0.0],\n",
    "           [10.0, 10.0, 10.0],\n",
    "           [20.0, 20.0, 20.0],\n",
    "           [30.0, 30.0, 30.0]])\n",
    "b = torch.tensor([1.0, 2.0, 3.0])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso temos as dimensões `a:4x3` e `b:1x3`:\n",
    "Ao percorrermos as dimensões de ambos temos\n",
    "- A segunda dimensão é igual a 3: `2i`  \n",
    "- A primeira dimensão de um dos tensores é 1: `2ii`\n",
    "\n",
    "Assim a única linha do vetor `b` é replicada outras 3 vezes para que o resultado tenha as mesmas dimensão da matriz `a` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://numpy.org/devdocs/_images/theory.broadcast_2.gif \"Credit: Documentação do Numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _broadcasting_ em vários casos não é intuitivo, esses foram dois exemplos super simples e somente a utilização constante e botar a mão na massa com vários exemplos é que torna o conceito um pouco mais claro. Consulte esse [link](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) a seguir para um tutorial detalhado sobre broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicação de Matrizes\n",
    "A definição de Multiplicação de 2 matrizes A e B é:\n",
    "![](images/matmul.jpeg \"Credit: https://pt.overleaf.com/articles/matrix-multiplication-revised/qkvxttgrsrqh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fórmula para cada termo da matriz produto C é:\n",
    "$$C_{ij} = \\sum_kA_{ik}B_{kj}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em código no geral é mais fácil visualizar, vamos criar 2 matrizes de tamanho razoável para analisarmos a eficiência dos cálculos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(56, 112)\n",
    "b = torch.randn(112, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensões\n",
    "a_linhas, a_colunas = a.shape \n",
    "b_linhas, b_colunas = b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que seja possível multiplicar ambas as matrizes precisamos que o número de colunas da matriz `a` deve ser igual ao número de linhas da coluna `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_colunas == b_linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inicialmente criar o vetor produto vazio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros(a_linhas, b_colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na definição básica da multiplicação de matriz temos 3 loops:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(a_linhas):\n",
    "    for j in range(b_colunas):\n",
    "        for k in range(a_colunas): # ou b_linhas\n",
    "            c[i,j] += a[i,k] * b[k,j]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos colocar a definição acima numa função, assim não repetimos código, podemos reutilizá-lo e mensurar o tempo médio de execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a:torch.Tensor,b:torch.Tensor)-> torch.Tensor:\n",
    "    \"Retorna a matrix produto entre a e b\"\n",
    "    # Dimensões\n",
    "    a_linhas, a_cols = a.shape \n",
    "    b_linhas, b_cols = b.shape\n",
    "    \n",
    "    # Verificação de Compatibilidade\n",
    "    assert a_cols==b_linhas, \"O número de colunas da matriz {a_cols} deve ser igual a {b_linhas}\"\n",
    "    \n",
    "    c = torch.zeros(a_linhas, b_cols)\n",
    "    \n",
    "    for i in range(a_linhas):\n",
    "        for j in range(b_cols):\n",
    "            for k in range(a_cols): # ou b_linhas\n",
    "                c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular quanto tempo leva esse cálculo utilizando a definição básica com 3 loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 0 ns, total: 13.4 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%time c=matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que essas matrizes, muito pequenas para os parâmetros modermos de Deep Learning, o cálculo já leva um tempo enorme, o que a torna insustentável. Na prática os \"loops\" são eliminados, e métodos eficientes de \"vetorização\" são utilizados. Vamos testar como podemos otimizar o código somente utilizando o _broadcasting_ visto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos eliminar o índice `k`, usando a multiplicação elemento a elemento, no lugar do índice `k` colocamos `:`, o que significa aplicarmos em todo eixo referenciado. A seguir utilizamos o método `sum` no eixo. Isso resulta na mesma operação efetuada anteriormente com o loop em `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a:torch.Tensor,b:torch.Tensor)-> torch.Tensor:\n",
    "    \"Retorna a matrix produto entre a e b\"\n",
    "    # Dimensões\n",
    "    a_linhas, a_cols = a.shape \n",
    "    b_linhas, b_cols = b.shape\n",
    "    \n",
    "    # Verificação de Compatibilidade\n",
    "    assert a_cols==b_linhas, \"O número de colunas da matriz {a_cols} deve ser igual a {b_linhas}\"\n",
    "    \n",
    "    c = torch.zeros(a_linhas, b_cols)\n",
    "    for i in range(a_linhas):\n",
    "        for j in range(b_cols):    \n",
    "            c[i,j] = (a[i,:] * b[:,j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 139 µs, total: 137 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%time c=matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos otimizar mais e eliminar o loop no índice `j`. No entanto não podemos simplesmente eliminá-lo, porque senão viola as regras acima de _broadcasting_, seja por exemplo o índice `i=0` e `j=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (112) must match the size of tensor b (56) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c7015e3e45b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (112) must match the size of tensor b (56) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "a[0, :] * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 56])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O vetor `a` indexado possui somente uma dimensão com 4 elementos e `b` possui 2 dimensões no formato 4x3. Para nos adequar às regras de _broadcasting_ temos que criar um eixo adicional para assim atender à regra `2i`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tal podemos indexar o eixo adicional com o valor especial `None` ou usar o método `unsqueeze()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, :, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a:torch.Tensor,b:torch.Tensor)-> torch.Tensor:\n",
    "    \"Retorna a matrix produto entre a e b\"\n",
    "    # Dimensões\n",
    "    a_linhas, a_cols = a.shape \n",
    "    b_linhas, b_cols = b.shape\n",
    "    \n",
    "    # Verificação de Compatibilidade\n",
    "    assert a_cols==b_linhas\n",
    "    \n",
    "    c = torch.zeros(a_linhas, b_cols)\n",
    "    for i in range(a_linhas):\n",
    "        c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 ms, sys: 0 ns, total: 3.98 ms\n",
      "Wall time: 3.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time c=matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas isso foi somente para ilustrar os conceitos, na prática usamos a Implementação Otimizada do Pytorch em C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 µs, sys: 3.9 ms, total: 4.07 ms\n",
      "Wall time: 3.39 ms\n"
     ]
    }
   ],
   "source": [
    "%time c= a.matmul(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um método equivalente é usar o operador `@`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 µs, sys: 5 µs, total: 148 µs\n",
      "Wall time: 154 µs\n"
     ]
    }
   ],
   "source": [
    "%time c= a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso foi somente uma pincelada super básica sobre tensores e alguns conceitos e operações básicas sobre eles e uma ilustração em como implementar multiplicação de matrizes de forma básica e mais otimizada  para ilustrar tais conceitos. Por fim simplesmente utilizamos a implementação nativa do Pytorch quando formos de fato utilizar tais operações  "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "pt",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "pt",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
